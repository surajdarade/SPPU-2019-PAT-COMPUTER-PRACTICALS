{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fdcb91b-768c-4102-90cb-7a19e45b64b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Analytics \n",
      "1. Extract Sample document and apply following document preprocessing methods: \n",
      "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization. \n",
      "2. Create representation of document by calculating Term Frequency and Inverse Document \n",
      "Frequency. \n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"Text Analytics \n",
    "1. Extract Sample document and apply following document preprocessing methods: \n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization. \n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document \n",
    "Frequency. \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99569cb4-2b5c-45d9-91de-9b80a3d6ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b73bda40-a36f-42ac-9843-0083cbaae00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\panka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\panka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\panka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\panka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a7846da-cb7c-44e1-a6eb-36f35bcabaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Analytics for Document :\n",
      "\n",
      "Tokenization (Word):\n",
      "['Text', 'analytics', 'involves', 'analyzing', 'and', 'processing', 'text', 'data', 'to', 'extract', 'meaningful', 'insights', '.', 'It', 'includes', 'tasks', 'such', 'as', 'tokenization', ',', 'POS', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'lemmatization', ',', 'and', 'more', '.', 'Text', 'analytics', 'is', 'widely', 'used', 'in', 'natural', 'language', 'processing', '(', 'NLP', ')', ',', 'sentiment', 'analysis', ',', 'information', 'retrieval', ',', 'and', 'other', 'applications', '.']\n",
      "\n",
      "Tokenization (Sentence):\n",
      "['Text analytics involves analyzing and processing text data to extract meaningful insights.', 'It includes tasks such as tokenization, POS tagging, stop words removal, stemming, lemmatization, and more.', 'Text analytics is widely used in natural language processing (NLP), sentiment analysis, information retrieval, and other applications.']\n",
      "\n",
      "POS Tagging:\n",
      "[('Text', 'JJ'), ('analytics', 'NNS'), ('involves', 'VBZ'), ('analyzing', 'VBG'), ('and', 'CC'), ('processing', 'VBG'), ('text', 'NN'), ('data', 'NNS'), ('to', 'TO'), ('extract', 'VB'), ('meaningful', 'JJ'), ('insights', 'NNS'), ('.', '.'), ('It', 'PRP'), ('includes', 'VBZ'), ('tasks', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('tokenization', 'NN'), (',', ','), ('POS', 'NNP'), ('tagging', 'NN'), (',', ','), ('stop', 'VB'), ('words', 'NNS'), ('removal', 'JJ'), (',', ','), ('stemming', 'VBG'), (',', ','), ('lemmatization', 'NN'), (',', ','), ('and', 'CC'), ('more', 'JJR'), ('.', '.'), ('Text', 'NN'), ('analytics', 'NNS'), ('is', 'VBZ'), ('widely', 'RB'), ('used', 'VBN'), ('in', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('(', '('), ('NLP', 'NNP'), (')', ')'), (',', ','), ('sentiment', 'JJ'), ('analysis', 'NN'), (',', ','), ('information', 'NN'), ('retrieval', 'NN'), (',', ','), ('and', 'CC'), ('other', 'JJ'), ('applications', 'NNS'), ('.', '.')]\n",
      "\n",
      "Filtered Tokens (Stop Words Removal):\n",
      "['Text', 'analytics', 'involves', 'analyzing', 'processing', 'text', 'data', 'extract', 'meaningful', 'insights', '.', 'includes', 'tasks', 'tokenization', ',', 'POS', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'lemmatization', ',', '.', 'Text', 'analytics', 'widely', 'used', 'natural', 'language', 'processing', '(', 'NLP', ')', ',', 'sentiment', 'analysis', ',', 'information', 'retrieval', ',', 'applications', '.']\n",
      "\n",
      "Stemmed Tokens:\n",
      "['text', 'analyt', 'involv', 'analyz', 'process', 'text', 'data', 'extract', 'meaning', 'insight', '.', 'includ', 'task', 'token', ',', 'po', 'tag', ',', 'stop', 'word', 'remov', ',', 'stem', ',', 'lemmat', ',', '.', 'text', 'analyt', 'wide', 'use', 'natur', 'languag', 'process', '(', 'nlp', ')', ',', 'sentiment', 'analysi', ',', 'inform', 'retriev', ',', 'applic', '.']\n",
      "\n",
      "Lemmatized Tokens:\n",
      "['Text', 'analytics', 'involves', 'analyzing', 'processing', 'text', 'data', 'extract', 'meaningful', 'insight', '.', 'includes', 'task', 'tokenization', ',', 'POS', 'tagging', ',', 'stop', 'word', 'removal', ',', 'stemming', ',', 'lemmatization', ',', '.', 'Text', 'analytics', 'widely', 'used', 'natural', 'language', 'processing', '(', 'NLP', ')', ',', 'sentiment', 'analysis', ',', 'information', 'retrieval', ',', 'application', '.']\n",
      "\n",
      "TF-IDF Representation:\n",
      "   analysis  analytics  analyzing       and  applications        as      data  \\\n",
      "0  0.129099   0.258199   0.129099  0.387298      0.129099  0.129099  0.129099   \n",
      "\n",
      "    extract        in  includes  ...      stop      such   tagging     tasks  \\\n",
      "0  0.129099  0.129099  0.129099  ...  0.129099  0.129099  0.129099  0.129099   \n",
      "\n",
      "       text        to  tokenization      used    widely     words  \n",
      "0  0.387298  0.129099      0.129099  0.129099  0.129099  0.129099  \n",
      "\n",
      "[1 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "def textAnalysis(text):\n",
    "    tokens_word = word_tokenize(text)\n",
    "    tokens_sent = sent_tokenize(text)\n",
    "\n",
    "    pos_tags = pos_tag(tokens_word)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens_word if word.lower() not in stop_words]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
    "\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "    print(\"\\nTokenization (Word):\")\n",
    "    print(tokens_word)\n",
    "    print(\"\\nTokenization (Sentence):\")\n",
    "    print(tokens_sent)\n",
    "    print(\"\\nPOS Tagging:\")\n",
    "    print(pos_tags)\n",
    "    print(\"\\nFiltered Tokens (Stop Words Removal):\")\n",
    "    print(filtered_tokens)\n",
    "    print(\"\\nStemmed Tokens:\")\n",
    "    print(stemmed_tokens)\n",
    "    print(\"\\nLemmatized Tokens:\")\n",
    "    print(lemmatized_tokens)\n",
    "    print(\"\\nTF-IDF Representation:\")\n",
    "    print(tfidf_df)\n",
    "\n",
    "file_path = \"file.txt\"\n",
    "\n",
    "# Read text from the files\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Perform text analytics tasks on each text\n",
    "print(\"Text Analytics for Document :\")\n",
    "textAnalysis(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
